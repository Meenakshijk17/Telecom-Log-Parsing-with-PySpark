# Telecom Log Parsing with Spark SQL

This project aims to develop a solution using Apache Spark to parse and analyze log files from various components of a telecom application. The objective is to develop a prototype that can monitor these components in a production environment by detecting warnings or exceptions within the logs.

## Dataset Overview

The dataset comprises log files generated by different components of the telecom application.

## Tasks

### Part 1 - Without using Data Frames

1. **Load data into Spark:**
   - Load the log data into Spark for further processing.

2. **Count 404 HTTP codes in access logs:**
   - Identify and count occurrences of HTTP 404 status codes in the access logs.

3. **Identify broken URLs:**
   - Determine which URLs are broken based on the log data.

4. **Verify absence of null columns:**
   - Confirm that there are no null columns present in the original dataset.

5. **Replace null values:**
   - Replace any null values in the dataset with constants such as 0.

6. **Parse timestamp to readable date:**
   - Convert the timestamp in the log data to a human-readable date format.

7. **Describe HTTP status values and their frequencies:**
   - Provide a summary of the HTTP status values present in the data along with their respective frequencies.

8. **Calculate unique hosts and their average requests:**
   - Determine the number of unique hosts in the entire log and calculate their average request count.

### Part 2 - Using Data Frames

1. **Load data into Spark Data Frame:**
   - Repeat the process of loading log data into Spark Data Frame.

2. **Count 404 HTTP codes in access logs:**
   - Repeat the task of identifying and counting occurrences of HTTP 404 status codes.

3. **Identify broken URLs:**
   - Repeat the task of determining broken URLs based on the log data.

4. **Verify absence of null columns:**
   - Confirm again that there are no null columns present in the dataset.

5. **Replace null values:**
   - Repeat the task of replacing null values with constants.

6. **Parse timestamp to readable date:**
   - Convert the timestamp in the log data to a human-readable date format, similar to Part 1.

7. **Describe HTTP status values and their frequencies:**
   - Provide a summary of HTTP status values and their frequencies, similar to Part 1.

8. **Calculate unique hosts and their average requests:**
   - Repeat the task of determining the number of unique hosts and calculating their average request count.

## Conclusion

This project involves leveraging PySpark and regular expressions to efficiently process and analyze large volumes of log data from telecom components. By implementing the outlined tasks, it is aimed to gain insights into system performance and identify potential issues for effective monitoring and maintenance.